from fastapi import APIRouter
from typing import Dict, Any
from ..schemas import GenerateReq
# ì•„ë˜ ìœ í‹¸/ì„œë¹„ìŠ¤ëŠ” í˜„ main.pyì™€ ë™ì¼ import ê²½ë¡œ ìœ ì§€
from ..repo import sample_nonfiction
from ..openai_client import llm_generate_with_evidence, llm_quality, call_json, embed_texts
from ..mapping_verify import verify_with_evidence
import re
import hashlib
import unicodedata
# ğŸ‘‰ ê¸°ì¡´ app/main.pyì— ìˆë˜ ìƒìˆ˜/ìœ í‹¸ ì¼ë¶€ë¥¼ ì´ íŒŒì¼ë¡œ ë³µì‚¬
# (DIFF_TO_GRADE, SIM_THRESHOLD, trim_evidence_by_similarity ë“±)

from fastapi import FastAPI
from pydantic import BaseModel, Field
from typing import List, Dict, Any
import math
from app.routers import rag_similar

DIFF_TO_GRADE = {"ê¸°ì´ˆ": "ê³ 1", "ë³´í†µ": "ê³ 2", "ì–´ë ¤ì›€": "ê³ 3"}

MAX_REPAIR_ROUNDS = 2
MAX_REGENERATE   = 1

USE_EMBED_RERANK = True
SIM_THRESHOLD    = 0.22

def rewrite_choice_with_evidence(passage_sentences: List[Dict[str,str]], choice: Dict[str,Any], must: str) -> str:
    sents = {s["id"]: s["text"] for s in passage_sentences}
    evidence = {sid: sents.get(sid,"") for sid in choice.get("evidence_sent_ids", [])}
    prompt = f"""
ì—­í• : êµ­ì–´ ë¹„ë¬¸í•™ ì„ ì§€ êµì •ê¸°. JSONë§Œ.
ìš”êµ¬: ì•„ë˜ [ê·¼ê±° ë¬¸ì¥]ì— ì§ì ‘ ì—°ê²°ë˜ë„ë¡ ì„ ì§€ í…ìŠ¤íŠ¸ë¥¼ ìµœì†Œ ìˆ˜ì •í•˜ë¼.
- ëª©í‘œ íŒì •: {must}
- ìƒˆë¡œìš´ ì‚¬ì‹¤/ìš©ì–´ ì¶”ê°€ ê¸ˆì§€, ê·¼ê±° ë¬¸ì¥ì˜ ë‚´ìš©/ê´€ê³„ë¥¼ ëª…í™•íˆ ë°˜ì˜
ì¶œë ¥: {{"text":"..."}}

[ê·¼ê±° ë¬¸ì¥]: {evidence}
[ì› ì„ ì§€]: {choice.get("text","")}
"""
    out = call_json(prompt) or {}
    new_text = out.get("text") or choice.get("text","")
    return new_text.strip()

def make_db_key(title: str, passage_text: str) -> str:
    """
    - í•œê¸€/ì˜ë¬¸/ìˆ«ì/í•˜ì´í”ˆë§Œ í—ˆìš©, ê³µë°±ì€ í•˜ì´í”ˆìœ¼ë¡œ
    - ì†Œë¬¸ìí™”(ì˜ë¬¸ë§Œ), ì–‘ë í•˜ì´í”ˆ ì •ë¦¬
    - ë³¸ë¬¸ sha1 ì• 8ê¸€ìë¥¼ suffixë¡œ ë¶™ì—¬ ì¶©ëŒ ìµœì†Œí™”
    """
    if not title:
        title = (passage_text[:20] or "untitled").strip()

    t = unicodedata.normalize("NFKC", title)
    t = re.sub(r"\s+", "-", t)                 # ê³µë°± -> -
    t = re.sub(r"[^ê°€-í£A-Za-z0-9\-]", "", t)  # í—ˆìš© ì™¸ ì œê±°
    t = re.sub(r"-{2,}", "-", t).strip("-")    # ì¤‘ë³µ/ì–‘ë -
    t = t.lower()

    h = hashlib.sha1((passage_text or "").encode("utf-8")).hexdigest()[:8]
    return f"{t}-{h}" if t else f"untitled-{h}"

def trim_evidence_by_overlap(choice_text: str, sentences: List[Dict[str, str]], ev_ids: List[int], max_keep: int = 2) -> List[int]:
    valid_ids = {int(s["id"]) for s in sentences}
    ev_ids = [int(i) for i in ev_ids if int(i) in valid_ids]
    if not ev_ids: return []
    toks = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", choice_text or ""))
    by_id = {int(s["id"]): s["text"] for s in sentences}
    scored = []
    for sid in ev_ids:
        st = by_id.get(sid, "")
        score = sum(1 for t in toks if t and t in st)
        scored.append((score, sid))
    picked = [sid for score, sid in sorted(scored, key=lambda x: x[0], reverse=True) if score > 0]
    if not picked: return ev_ids[:1]
    return picked[:max_keep]

# [EMBED] ì½”ì‚¬ì¸
def _cosine(a, b):
    dot = sum(x*y for x, y in zip(a, b))
    na = math.sqrt(sum(x*x for x in a)) or 1e-9
    nb = math.sqrt(sum(y*y for y in b)) or 1e-9
    return dot / (na * nb)

def trim_evidence_by_similarity(choice_text: str, sentences: List[Dict[str, str]], ev_ids: List[int], max_keep: int = 2, min_sim: float = SIM_THRESHOLD):
    """
    1) choice_text + í›„ë³´ evidence ë¬¸ì¥ë“¤ ì„ë² ë”©
    2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ìƒìœ„ max_keep ì„ íƒ
    3) min_sim ë¯¸ë§Œì€ íƒˆë½(ì „ë¶€ ë¯¸ë‹¬ì´ë©´ ìµœê³  1ê°œ í´ë°±)
    ë°˜í™˜: (ì„ íƒ evidence id ë¦¬ìŠ¤íŠ¸, ì§„ë‹¨ dict)
    """
    id2sent = {int(s["id"]): s["text"] for s in sentences}
    ev_ids = [int(i) for i in ev_ids if int(i) in id2sent]
    if not ev_ids:
        return [], {"method":"embed", "picked":[], "sims":{}}

    texts = [choice_text] + [id2sent[i] for i in ev_ids]
    embs = embed_texts(texts)

    if not embs or len(embs) != len(texts):  # ì„ë² ë”© ì‹¤íŒ¨ â†’ overlap í´ë°±
        picked = trim_evidence_by_overlap(choice_text, sentences, ev_ids, max_keep=max_keep)
        return picked, {"method":"overlap_fallback", "picked":picked, "sims":{}}

    q = embs[0]
    sims = {}
    for idx, sid in enumerate(ev_ids, start=1):
        sims[sid] = _cosine(q, embs[idx])

    ranked = [sid for sid, s in sorted(sims.items(), key=lambda x: x[1], reverse=True) if s >= min_sim]
    if not ranked:
        ranked = [max(sims.items(), key=lambda x: x[1])[0]]  # ì „ë¶€ ë¯¸ë‹¬ ì‹œ ìµœê³  1ê°œ

    picked = ranked[:max_keep]
    return picked, {"method":"embed", "picked":picked, "sims":{str(k): round(v,4) for k,v in sims.items()}}


router = APIRouter(tags=["items"])

# ===== ì—¬ê¸°ì— app/main.pyì˜ generate() ë³¸ë¬¸ì„ ê·¸ëŒ€ë¡œ ì˜®ê²¨ ë¶™ì´ì„¸ìš” =====
# ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œë§Œ ë°”ê¿‰ë‹ˆë‹¤: /api/v1/items/generate
@router.post("/items/generate")
def generate(req: GenerateReq):
    # 0) ë‚œì´ë„â†’í•™ë…„/ì£¼ì œ í•„í„°ë¡œ ë² ì´ìŠ¤ ì§€ë¬¸ ìƒ˜í”Œ
    base = sample_nonfiction(
        grade_level=DIFF_TO_GRADE.get(req.difficulty),
        topic=req.topic
    )
    key_points = "; ".join([s["text"] for s in base["sentences"][:4]]) if base else ""

    # 1) ìƒì„±(ê·¼ê±° ë‚´ì¥)
    gen = llm_generate_with_evidence(
        req.mode, topic=req.topic, difficulty=req.difficulty, target_chars=req.target_chars
    )
    passage_sentences = gen.get("passage_sentences", [])
    # ë°©ì–´: id ë³´ì •
    for i, s in enumerate(passage_sentences):
        s["id"] = int(s.get("id") or (i+1))
        s["text"] = str(s.get("text","")).strip()
    passage_text = " ".join(s["text"] for s in passage_sentences if s["text"])

    # â˜… ì œëª© ë° DB Key
    title = (gen.get("title") or "").strip()
    db_key = make_db_key(title, passage_text)
    question = (gen.get("question") or "ìœ„ ê¸€ì˜ ë‚´ìš©ìœ¼ë¡œ ì ì ˆí•œ ê²ƒì„ ê³ ë¥´ì‹œì˜¤.").strip()

    result = {
        "title": title,
        "question": question,   
        "db_key": db_key,
        "generated_passage": passage_text,
        "sentences": passage_sentences,
        "quality": llm_quality(passage_text, req.topic, key_points),
        "repairs": [],
        "regen_count": 0,
        "difficulty": req.difficulty,
        "topic": req.topic,
        "target_chars": req.target_chars,
        # â˜… ì—¬ê¸° ì¶”ê°€
        "base_group_id": base.get("group_id") if base else None
    }
    if req.mode.upper() != "B":
        return result

    choices = gen.get("choices", []) or []

    def accept_all(_choices):
        verified_map = {}
        sent_id_set = {int(s["id"]) for s in passage_sentences}

        # [RAGAS-like] ëˆ„ì ìš©
        rag_scores = []
        prec_cnt_total, prec_den_total = 0, 0
        rec_cnt_total,  rec_den_total  = 0, 0
        faith_err_cnt = 0

        for idx, c in enumerate(_choices):
            original_ev = list(c.get("evidence_sent_ids") or [])  # ì›ë˜ LLMê°€ ì§€ì •í•œ ê·¼ê±° ì„¸íŠ¸
            ev = [int(x) for x in original_ev if int(x) in sent_id_set]

            # ê·¼ê±° ì •ì œ(ì„ë² ë”© ë¦¬ë­í¬ or overlap)
            if USE_EMBED_RERANK:
                ev, diag = trim_evidence_by_similarity(c.get("text",""), passage_sentences, ev, max_keep=2, min_sim=SIM_THRESHOLD)
            else:
                ev = trim_evidence_by_overlap(c.get("text",""), passage_sentences, ev, max_keep=2)
                diag = {"method":"overlap", "picked":ev, "sims":{}}

            if not ev:
                return False, idx, "empty evidence", verified_map

            c["evidence_sent_ids"] = ev
            c["evidence_diag"] = diag

            # ê·¼ê±° ê°•ë„(í‰ê· /ìµœì†Ÿê°’)
            sim_vals = [diag.get("sims",{}).get(str(i)) for i in ev]
            sim_vals = [v for v in sim_vals if isinstance(v, (int,float))]
            evidence_strength = round(sum(sim_vals)/len(sim_vals),4) if sim_vals else 0.0
            evidence_min = round(min(sim_vals),4) if sim_vals else 0.0

            # ê²€ì¦
            must = "support" if c.get("is_correct") else "contradict"
            v = verify_with_evidence(passage_sentences, c.get("text",""), ev, must)
            c["verify"] = v
            label = v.get("label")

            # [RAGAS-like] context_precision / recall / faithfulness_error
            # - precision: ìµœì¢… ì„ íƒ ev ì¤‘ ìœ ì‚¬ë„ ì„ê³„ ì´ìƒ ë¹„ìœ¨
            good_ev = [i for i in ev if isinstance(diag.get("sims",{}).get(str(i)), (int,float)) and diag["sims"][str(i)] >= SIM_THRESHOLD]
            prec_cnt_total += len(good_ev)
            prec_den_total += max(1, len(ev))  # 0ë¶„ëª¨ ë°©ì§€

            # - recall: ì›ë˜ evidence_sent_ids ì¤‘ ìµœì¢… ë‚¨ì€ ë¹„ìœ¨
            orig_ev_clean = [int(x) for x in original_ev if int(x) in sent_id_set]
            inter = set(orig_ev_clean).intersection(set(ev))
            rec_cnt_total += len(inter)
            rec_den_total += max(1, len(orig_ev_clean))

            # - faithfulness error: ëª©í‘œ ë¼ë²¨ ë¶ˆì¼ì¹˜ ë˜ëŠ” weak/no_evidence
            if label != must or label in ("weak", "no_evidence"):
                faith_err_cnt += 1

            verified_map[idx] = {
                "final_sent_ids": ev,
                "label": label,
                "notes": v.get("notes",""),
                "evidence_strength": evidence_strength,
                "evidence_min": evidence_min,
                "diag": diag,
                # ì§€ë¬¸/ì„ ì§€ ë‹¨ìœ„ì˜ ì†Œì§€í‘œ(ì°¸ê³ ìš©)
                "context_precision_local": round(len(good_ev) / max(1, len(ev)), 3),
                "context_recall_local": round(len(inter) / max(1, len(orig_ev_clean)), 3),
            }

            rag_scores.append({
                "idx": idx,
                "must": must,
                "label_ok": (label == must),
                "evidence_strength": evidence_strength,
                "evidence_min": evidence_min
            })

            if label != must:
                return False, idx, f"need {must}, got {label}", verified_map

        # ì„¸íŠ¸ ìš”ì•½ ì§€í‘œ
        acc = sum(1 for r in rag_scores if r["label_ok"]) / max(1,len(rag_scores))
        avg_strength = sum(r["evidence_strength"] for r in rag_scores)/max(1,len(rag_scores))

        context_precision = round(prec_cnt_total / max(1, prec_den_total), 3)
        context_recall    = round(rec_cnt_total  / max(1, rec_den_total),  3)
        faithfulness_error_rate = round(faith_err_cnt / max(1, len(_choices)), 3)

        result_summary = {
            "label_accuracy": round(acc,3),
            "avg_evidence_strength": round(avg_strength,4),
            "context_precision": context_precision,
            "context_recall": context_recall,
            "faithfulness_error_rate": faithfulness_error_rate
        }
        return True, None, result_summary, verified_map

    ok, bad_idx, reason_or_summary, verified_map = accept_all(choices)
    repair_round, regen = 0, 0

    while not ok and repair_round < MAX_REPAIR_ROUNDS:
        must = "support" if choices[bad_idx].get("is_correct") else "contradict"
        before = choices[bad_idx].get("text","")
        choices[bad_idx]["text"] = rewrite_choice_with_evidence(passage_sentences, choices[bad_idx], must)
        result["repairs"].append({
            "index": bad_idx, "must": must,
            "before": before, "after": choices[bad_idx]["text"],
            "reason": reason_or_summary
        })

        ok, bad_idx, reason_or_summary, verified_map = accept_all(choices)
        repair_round += 1

        if not ok and repair_round >= MAX_REPAIR_ROUNDS and regen < MAX_REGENERATE:
            # ì„¸íŠ¸ ì¬ìƒì„±
            regen += 1
            result["regen_count"] = regen
            gen = llm_generate_with_evidence(req.mode, topic=req.topic, difficulty=req.difficulty, target_chars=req.target_chars)
            passage_sentences = gen.get("passage_sentences", [])
            for i, s in enumerate(passage_sentences):
                s["id"] = int(s.get("id") or (i+1))
                s["text"] = str(s.get("text","")).strip()
            passage_text = " ".join(s["text"] for s in passage_sentences if s["text"])
            result["generated_passage"] = passage_text
            result["sentences"] = passage_sentences
            choices = gen.get("choices", []) or []
            repair_round = 0
            ok, bad_idx, reason_or_summary, verified_map = accept_all(choices)

    result["choices"] = choices
    result["verified_mapping"] = verified_map
    # [RAGAS-like] ì„¸íŠ¸ ìš”ì•½ ì§€í‘œ ë°˜ì˜
    if isinstance(reason_or_summary, dict):
        result["rag_eval"] = {
            "label_accuracy": reason_or_summary.get("label_accuracy"),
            "avg_evidence_strength": reason_or_summary.get("avg_evidence_strength"),
            "context_precision": reason_or_summary.get("context_precision"),
            "context_recall": reason_or_summary.get("context_recall"),
            "faithfulness_error_rate": reason_or_summary.get("faithfulness_error_rate"),
            "sim_threshold": SIM_THRESHOLD,
            "method": "embed_rerank" if USE_EMBED_RERANK else "overlap"
        }
    else:
        result["rag_eval"] = {"method": "embed_rerank" if USE_EMBED_RERANK else "overlap"}

    return result